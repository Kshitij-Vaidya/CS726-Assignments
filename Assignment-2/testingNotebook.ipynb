{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kshitij-vaidya/miniconda3/envs/cs726env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Optional, Union, List, Tuple\n",
    "from helperClasses import TimeEmbedding, UNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, numChannels: int):\n",
    "        super().__init__()\n",
    "        # numChannels is the number of channels in the input\n",
    "        self.numChannels = numChannels\n",
    "        # Define the layers of the Multi Layer Perceptron (MLP) Model\n",
    "        self.linear1 = nn.Linear(self.numChannels // 4, \n",
    "                                 self.numChannels)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.linear2 = nn.Linear(self.numChannels, \n",
    "                                 self.numChannels)\n",
    "    \n",
    "    def forward(self, timeInput: torch.Tensor):\n",
    "        halfDim = self.numChannels // 8\n",
    "        embeddings = math.log(10000) / (halfDim - 1)\n",
    "        embeddings = torch.exp(torch.arange(halfDim) * -embeddings).to(timeInput.device)\n",
    "        embeddings = timeInput[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        # Pass the embeddings through the linear and activation layers\n",
    "        embeddings = self.linear1(embeddings)\n",
    "        embeddings = self.activation(embeddings)\n",
    "        embeddings = self.linear2(embeddings)\n",
    "        # Return the final embeddings of the time input\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Define the Residual Block class that has convolution layers with group normalisation\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inputChannels: int, outputChannels: int,\n",
    "                 timeChannels: int, numGroups: int = 32, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Define group normalisation and the first layer\n",
    "        print(f'Input Channels : {inputChannels}')\n",
    "        print(f'Output Channels : {outputChannels}')\n",
    "        print(f'Number of Groups : {numGroups}')\n",
    "        self.norm1 = nn.GroupNorm(numGroups, inputChannels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.convLayer1 = nn.Conv2d(inputChannels, outputChannels,\n",
    "                                    kernel_size=3, padding=1)\n",
    "        # Define the second layer with group normalisation\n",
    "        self.norm2 = nn.GroupNorm(numGroups, outputChannels)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.convLayer2 = nn.Conv2d(outputChannels, outputChannels,\n",
    "                                    kernel_size=3, padding=1)\n",
    "        \n",
    "        # If the input and output layers are not the same, then use a shortcut connection\n",
    "        if inputChannels != outputChannels:\n",
    "            self.shortcut = nn.Conv2d(inputChannels, outputChannels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Add the linear layer for the time embeddings\n",
    "        self.timeEmbedding = nn.Linear(timeChannels, outputChannels)\n",
    "        self.timeActivation = nn.SiLU()\n",
    "        # Define the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, time: torch.Tensor):\n",
    "        # Passing the input through the first layer\n",
    "        outLayer1 = self.norm1(input)\n",
    "        outLayer1 = self.activation1(outLayer1)\n",
    "        outLayer1 = self.convLayer1(outLayer1)\n",
    "        # Add the time embeddings to the output of the first layer\n",
    "        outLayer1 = outLayer1 + self.timeEmbedding(self.timeActivation(time))[:, :, None, None]\n",
    "        # Pass the output through the second layer\n",
    "        outLayer2 = self.norm2(outLayer1)\n",
    "        outLayer2 = self.activation2(outLayer2)\n",
    "        outLayer2 = self.dropout(outLayer2)\n",
    "        outLayer2 = self.convLayer2(outLayer2)\n",
    "        # Add the shortcut connection\n",
    "        output = outLayer2 + self.shortcut(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Define the Attention Block \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, numChannels: int, numHeads: int = 1, headDim: int = None, numGroups = 32):\n",
    "        super().__init__()\n",
    "        if headDim is None:\n",
    "            headDim = numChannels\n",
    "        # Define the normalisation layer\n",
    "        self.norm = nn.GroupNorm(numGroups, numChannels)\n",
    "\n",
    "        self.projection = nn.Linear(numChannels, numHeads * headDim * 3)\n",
    "\n",
    "        self.output = nn.Linear(numHeads * headDim, numChannels)\n",
    "        # Scale for dot product attention\n",
    "        self.scale = headDim ** -0.5\n",
    "        self.numHeads = numHeads\n",
    "        self.headDim = headDim\n",
    "\n",
    "    def forward(self, input: torch.Tensor, time: Optional[torch.Tensor] = None):\n",
    "        print(f'Input Shape : {input.shape}')\n",
    "        batchSize, numChannels, height, width = input.shape \n",
    "        input = input.view(batchSize, numChannels, -1).permute(0, 2, 1)\n",
    "        temp = self.projection(input).view(batchSize, -1, self.numHeads, self.headDim * 3)\n",
    "\n",
    "        query, key, value = torch.chunk(temp, 3, dim=-1)\n",
    "        attention = torch.einsum('b i h d, b j h d -> b i j h', query, key) * self.scale\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "\n",
    "        result = torch.einsum('b i j h, b j h d -> b i h d', attention, value)\n",
    "        print(f'Result Shape : {result.shape}')\n",
    "        result = result.reshape(batchSize, -1, self.numHeads * self.headDim)\n",
    "        print(f'Reshaped Result Shape : {result.shape}')\n",
    "        result = self.output(result)\n",
    "        print(f'Result after Output : {result.shape}')\n",
    "        # Adding the skip connection\n",
    "        output = result + input\n",
    "        output = output.permute(0, 2, 1).view(batchSize, numChannels, height, width)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, inputChannels: int, outputChannels, timeChannels: int, hasAttention: bool):\n",
    "        super().__init__()\n",
    "        self.residualBlock = ResidualBlock(inputChannels, outputChannels, timeChannels)\n",
    "\n",
    "        if hasAttention:\n",
    "            self.attentionBlock = AttentionBlock(outputChannels)\n",
    "        else:\n",
    "            self.attentionBlock = nn.Identity()\n",
    "        \n",
    "    def forward(self, input: torch.Tensor, time: torch.Tensor):\n",
    "        input = self.residualBlock(input, time)\n",
    "        output = self.attentionBlock(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def   __init__(self, inputChannels: int, outputChannels: int, timeChannels: int, hasAttention: bool):\n",
    "        super().__init__()\n",
    "        self.residualBlock = ResidualBlock(inputChannels, outputChannels, timeChannels)\n",
    "        if hasAttention:\n",
    "            self.attentionBlock = AttentionBlock(outputChannels)\n",
    "        else:\n",
    "            self.attentionBlock = nn.Identity()\n",
    "    \n",
    "    def forward(self, input: torch.Tensor, time: torch.Tensor):\n",
    "        input = self.residualBlock(input, time)\n",
    "        output = self.attentionBlock(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    def __init__(self, numChannels: int, timeChannels: int):\n",
    "        super().__init__()\n",
    "        self.residualBlock1 = ResidualBlock(numChannels, numChannels, timeChannels)\n",
    "        self.attentionBlock = AttentionBlock(numChannels)\n",
    "        self.residualBlock2 = ResidualBlock(numChannels, numChannels, timeChannels)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor, time: torch.Tensor):\n",
    "        input = self.residualBlock1(input, time)\n",
    "        input = self.attentionBlock(input)\n",
    "        output = self.residualBlock2(input, time)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, numChannels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(numChannels, numChannels * 2, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, numChannels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(numChannels, numChannels // 2, kernel_size=4, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return self.conv(input)\n",
    "    \n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(self, imageChannels: int = 3, numChannels: int = 6,\n",
    "                 channelMultiplier: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 isAttention: Union[Tuple[bool, ...], List[bool]] = (False, False, True, True),\n",
    "                 numBlocks: int = 2):\n",
    "        super().__init__()\n",
    "        numResolutions = len(channelMultiplier)\n",
    "        self.imageProjection = nn.Conv2d(imageChannels, numChannels, kernel_size=3, padding=1)\n",
    "        self.timeEmbedding = TimeEmbedding(numChannels * 4)\n",
    "        downSampleBlocks = []\n",
    "\n",
    "        outputChannels = inputChannels = numChannels\n",
    "        for i in range(numResolutions):\n",
    "            outputChannels = inputChannels * channelMultiplier[i]\n",
    "            for _ in range(numBlocks):\n",
    "                downSampleBlocks.append(DownSampleBlock(inputChannels, outputChannels, numChannels * 4, isAttention[i]))\n",
    "                inputChannels = outputChannels\n",
    "            if i < numResolutions - 1:\n",
    "                downSampleBlocks.append(DownSample(inputChannels))\n",
    "        \n",
    "        self.downSampleBlocks = nn.ModuleList(downSampleBlocks)\n",
    "        self.middleBlock = MiddleBlock(outputChannels, numChannels * 4, )\n",
    "\n",
    "        upSampleBlocks = []\n",
    "        inputChannels = outputChannels\n",
    "        for i in reversed(range(numResolutions)):\n",
    "            outputChannels = inputChannels // channelMultiplier[i]\n",
    "            for _ in range(numBlocks):\n",
    "                upSampleBlocks.append(UpSampleBlock(inputChannels, outputChannels, numChannels * 4, isAttention[i]))\n",
    "                inputChannels = outputChannels\n",
    "            if i > 0:\n",
    "                upSampleBlocks.append(UpSample(inputChannels))\n",
    "        \n",
    "        self.upSampleBlocks = nn.ModuleList(upSampleBlocks)\n",
    "\n",
    "        self.groupNorm = nn.GroupNorm(8, numChannels)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.outputLayer = nn.Conv2d(numChannels, imageChannels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, image: torch.Tensor, time: torch.Tensor):\n",
    "        image = self.imageProjection(image)\n",
    "        time = self.timeEmbedding(time)\n",
    "        skipConnectionOutputs = [image]\n",
    "\n",
    "        for downSampleBlock in self.downSampleBlocks:\n",
    "            image = downSampleBlock(image, time)\n",
    "            skipConnectionOutputs.append(image)\n",
    "        \n",
    "        image = self.middleBlock(image, time)\n",
    "\n",
    "        for upSampleBlock in self.upSampleBlocks:\n",
    "            if isinstance(upSampleBlock, UpSample):\n",
    "                image = upSampleBlock(image, time)\n",
    "            else:\n",
    "                skipConnection = skipConnectionOutputs.pop()\n",
    "                image = torch.cat([image, skipConnection], dim=1)\n",
    "                image = upSampleBlock(image, time)\n",
    "        \n",
    "        image = self.groupNorm(image)\n",
    "        image = self.activation(image)\n",
    "        image = self.outputLayer(image)\n",
    "        return image\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Instantiate and test the TimeEmbedding module\n",
    "    timeEmbed = TimeEmbedding(numChannels=128)\n",
    "    test_time_input = torch.randn(4)\n",
    "    time_output = timeEmbed(test_time_input)\n",
    "    print(\"Time embedding output shape:\", time_output.shape)\n",
    "\n",
    "    # Instantiate and test the ResidualBlock\n",
    "    resBlock = ResidualBlock(32, 64, 128)\n",
    "    res_input = torch.randn(1, 32, 16, 16)\n",
    "    res_time = torch.randn(1, 128)\n",
    "    res_output = resBlock(res_input, res_time)\n",
    "    print(\"Residual block output shape:\", res_output.shape)\n",
    "\n",
    "    # Instantiate and test the AttentionBlock\n",
    "    attBlock = AttentionBlock(numChannels=64, numHeads=2, headDim=32)\n",
    "    att_output = attBlock(res_output)\n",
    "    print(\"Attention block output shape:\", att_output.shape)\n",
    "\n",
    "    # Define the UNetModel and test it\n",
    "    unetModel = UNetModel()\n",
    "    test_image = torch.randn(1, 3, 64, 64)\n",
    "    test_time = torch.randn(1, 128)\n",
    "    output_image = unetModel(test_image, test_time)\n",
    "    print(\"Output image shape:\", output_image.shape)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, n_dim=3, n_steps=200, num_channels=128):\n",
    "        \"\"\"\n",
    "        Noise prediction network for the DDPM\n",
    "\n",
    "        Args:\n",
    "            n_dim: int, the dimensionality of the data\n",
    "            n_steps: int, the number of steps in the diffusion process\n",
    "        We have separate learnable modules for `time_embed` and `model`. `time_embed` can be learned or a fixed function as well\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.time_embed = TimeEmbedding(n_steps)\n",
    "        self.model = UNetModel(imageChannels=n_dim, numChannels=num_channels, numDownsampling=5, numBlocks=2, numIntermediateChannels=64, numResidualChannels=64, numFeatureChannels=64, numTopChannels=64, numOutputChannels=n_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor, the input data tensor [batch_size, n_dim]\n",
    "            t: torch.Tensor, the timestep tensor [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor, the predicted noise tensor [batch_size, n_dim]\n",
    "        \"\"\"\n",
    "        t = self.time_embed(t)\n",
    "        return self.model(x, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs726env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
