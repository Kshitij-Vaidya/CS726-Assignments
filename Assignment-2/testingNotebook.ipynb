{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler():\n",
    "    \"\"\"\n",
    "    Noise scheduler for the DDPM model\n",
    "\n",
    "    Args:\n",
    "        num_timesteps: int, the number of timesteps\n",
    "        type: str, the type of scheduler to use\n",
    "        **kwargs: additional arguments for the scheduler\n",
    "\n",
    "    This object sets up all the constants like alpha, beta, sigma, etc. required for the DDPM model\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_timesteps=50, type=\"linear\", **kwargs):\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.type = type\n",
    "\n",
    "        if type == \"linear\":\n",
    "            self.init_linear_schedule(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{type} scheduler is not implemented\") # change this if you implement additional schedulers\n",
    "\n",
    "\n",
    "    def init_linear_schedule(self, beta_start, beta_end):\n",
    "        \"\"\"\n",
    "        Precompute the linear schedule for beta, alpha, and other required quantities\n",
    "        \"\"\"\n",
    "        self.betas = torch.linspace(beta_start, beta_end, self.num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphaBar = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphaBarPrev = torch.cat([torch.tensor([1.0]), self.alphaBar[:-1]])\n",
    "        self.sqrtAlphaBar = torch.sqrt(self.alphaBar)\n",
    "        self.sqrtOneMinusAlphaBar = torch.sqrt(1.0 - self.alphaBar)\n",
    "        self.logOneMinusAlphaBar = torch.log(1.0 - self.alphaBar)\n",
    "        self.sqrtRecipAlphaBar = torch.sqrt(1.0 / self.alphaBar)\n",
    "        self.sqrtRecipMinusOneAlphaBar = torch.sqrt(1.0 / self.alphaBar - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 9.7959e-01, 9.3961e-01, 8.8208e-01, 8.1007e-01, 7.2741e-01,\n",
      "        6.3834e-01, 5.4715e-01, 4.5782e-01, 3.7373e-01, 2.9746e-01, 2.3068e-01,\n",
      "        1.7419e-01, 1.2798e-01, 9.1411e-02, 6.3428e-02, 4.2717e-02, 2.7897e-02,\n",
      "        1.7649e-02, 1.0805e-02, 6.3951e-03, 3.6543e-03, 2.0136e-03, 1.0684e-03,\n",
      "        5.4513e-04, 2.6700e-04, 1.2533e-04, 5.6269e-05, 2.4115e-05, 9.8430e-06,\n",
      "        3.8167e-06, 1.4020e-06, 4.8642e-07, 1.5883e-07, 4.8622e-08, 1.3892e-08,\n",
      "        3.6856e-09, 9.0261e-10, 2.0263e-10, 4.1352e-11, 7.5953e-12, 1.2400e-12,\n",
      "        1.7715e-13, 2.1692e-14, 2.2135e-15, 1.8069e-16, 1.1063e-17, 4.5154e-19,\n",
      "        9.2150e-21, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "tempNoiseScheduler = NoiseScheduler(50, type=\"linear\", beta_start=0.0, beta_end=1.0)\n",
    "print(tempNoiseScheduler.alphaBar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, n_dim=3, n_steps=200):\n",
    "        \"\"\"\n",
    "        Noise prediction network for the DDPM\n",
    "\n",
    "        Args:\n",
    "            n_dim: int, the dimensionality of the data\n",
    "            n_steps: int, the number of steps in the diffusion process\n",
    "        We have separate learnable modules for `time_embed` and `model`. `time_embed` can be learned or a fixed function as well\n",
    "\n",
    "        \"\"\"\n",
    "        super(DDPM, self).__init__()\n",
    "        self.time_embed = nn.Embedding(n_steps, n_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor, the input data tensor [batch_size, n_dim]\n",
    "            t: torch.Tensor, the timestep tensor [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor, the predicted noise tensor [batch_size, n_dim]\n",
    "        \"\"\"\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.model(x + t_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 73,  73, 181, 199,  49,   6,  57, 196,  58, 181])\n",
      "torch.Size([200, 3])\n",
      "tensor([[-0.1705,  0.0770, -0.3311],\n",
      "        [-0.2355, -0.0527, -0.2224],\n",
      "        [-0.6369, -0.2312, -0.0967],\n",
      "        [-0.3130, -0.1753, -0.9912],\n",
      "        [-0.2521, -0.1186, -0.3647],\n",
      "        [-0.2142,  0.0179, -0.1793],\n",
      "        [-0.3449,  0.1044, -1.3320],\n",
      "        [-0.2200, -0.0561, -0.3684],\n",
      "        [-0.3767, -0.1778, -0.5338],\n",
      "        [-0.7660, -0.2815, -0.0863]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testDDPM = DDPM()\n",
    "testInput = torch.randn(10, 3)\n",
    "testT = torch.randint(0, 200, (10,))\n",
    "# print(testInput)\n",
    "print(testT)\n",
    "forwardOutput = testDDPM.forward(testInput, testT)\n",
    "print(testDDPM.time_embed.weight.shape)\n",
    "print(forwardOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : DDPM, noise_scheduler : NoiseScheduler, dataloader, optimizer, epochs, run_name):\n",
    "    \"\"\"\n",
    "    Train the model and save the model and necessary plots\n",
    "\n",
    "    Args:\n",
    "        model: DDPM, model to train\n",
    "        noise_scheduler: NoiseScheduler, scheduler for the noise\n",
    "        dataloader: torch.utils.data.DataLoader, dataloader for the dataset\n",
    "        optimizer: torch.optim.Optimizer, optimizer to use\n",
    "        epochs: int, number of epochs to train the model\n",
    "        run_name: str, path to save the model\n",
    "    \"\"\"\n",
    "    prevLoss = float(\"inf\")\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    for epoch in range(epochs):\n",
    "        tqdmDataloader = tqdm(dataloader, desc = f\"Epoch : {epoch + 1}\")\n",
    "        for x, _ in tqdmDataloader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Fix the batch size and the number of timesteps\n",
    "            batch_size = x.shape[0]\n",
    "            t = torch.randint(0, noise_scheduler.num_timesteps, (batch_size,))\n",
    "\n",
    "            noise = torch.randn_like(x)\n",
    "            alphaBarT = noise_scheduler.alphaBar[t].unsqueeze(1).to(device)\n",
    "            sqrtAlphaBarT = noise_scheduler.sqrtAlphaBar[t].unsqueeze(1).to(device)\n",
    "            sqrtOneMinusAlphaBarT = noise_scheduler.sqrtOneMinusAlphaBar[t].unsqueeze(1).to(device)\n",
    "\n",
    "            xTilde = sqrtAlphaBarT * x + sqrtOneMinusAlphaBarT * noise\n",
    "            noisePred = model(xTilde, t)\n",
    "            loss = F.mse_loss(noisePred, noise)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tqdmDataloader.set_postfix({\"Loss\" : loss.item()})\n",
    "        if loss.item() < prevLoss:\n",
    "            # Save the model if the loss is less than the previous best loss\n",
    "            prevLoss = loss.item()\n",
    "            torch.save(model.state_dict(), \"models/\" + run_name + \".pth\")\n",
    "            print(f\"Model saved with loss {loss}\")\n",
    "        else:\n",
    "            print(f\"Model not saved with loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the albatross dataset\n",
    "moonsDataX, moonDatay = dataset.load_dataset(\"moons\")\n",
    "moonDataset = torch.utils.data.TensorDataset(moonsDataX, moonDatay.unsqueeze(1))\n",
    "moonDataloader = torch.utils.data.DataLoader(moonDataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1: 100%|██████████| 125/125 [00:00<00:00, 734.85it/s, Loss=0.516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with loss 0.5161635875701904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model with the given parameters\n",
    "model = DDPM(n_dim=2, n_steps=200)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, tempNoiseScheduler, moonDataloader, optimizer, 1, \"test_moons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs726env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
